{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1707645a-699e-4751-8da5-fa1307576fa1",
   "metadata": {},
   "source": [
    "# Проектна задача по предметот „Мобилна роботика“\n",
    "## Теодора Петровска, 98/2021\n",
    "#### Во оваа тетратка е решен 3Д проблем за слетување на ракета инспириран од LunarLander од OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136eba5-78b0-47f8-a9f7-ae7ee11819e5",
   "metadata": {},
   "source": [
    "Тестирање на работата на моторите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a115c-a29e-40b5-a2f9-e374f36b8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine_tester import run_test\n",
    "run_test(main=False, right=False, left=True, forward=False, back=False, steps=1000) # left thruster only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deba3fc-60d6-4dc4-95c3-221b7d726514",
   "metadata": {},
   "source": [
    "Импортирање на потребните библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d055317-880b-4730-b69d-35dc978c33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import statistics\n",
    "from collections import deque\n",
    "import threading\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import mujoco\n",
    "from mujoco import MjModel, MjData\n",
    "import mujoco_viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa663e7-a503-45e4-855c-08a41bb284e3",
   "metadata": {},
   "source": [
    "Дефинирање на хиперпараметрите\n",
    "- ALPHA: стапка на учење, колку брзо учи моделот\n",
    "- GAMMA: фактор на дисконтирање, ги балансира краткотрајните и долготрајните награди. 0.995 значи дека битни ни се идните награди\n",
    "- EPSILON*: го контролира истражувањето на агентот. Почни случајно да истражуваш, стани алчен со тек на време\n",
    "- BATCH SIZE: од колку примероци да учи одеднаш\n",
    "- BUFFER CAPACITY: максимална големина на меморијата\n",
    "- TARGET UPDATE FREQ: фреквенција на синхронизирање на посакуваниот модел. На секои 10 епизоди ги копираме (синхронизираме) тежините од policy_net (онаа што учи) до target_net (онаа што се користи за пресметување на целната Q-вредност).\n",
    "- TAU се користи за soft updates, бавно мешање на target и policy net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b27e333-501a-49b0-8f21-a855996768f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ALPHA = 1e-4\n",
    "GAMMA = 0.995\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_CAPACITY = 200_000\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "TAU = 0.005\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9d229-0935-49bb-8f46-fe28020753e7",
   "metadata": {},
   "source": [
    "Користиме Deep Q-Network (длабока Q мрежа) - вид на невронска мрежа што се користи за апроксимација на Q функцијата, којашто ни кажува колку е добро да се преземе акција а во состојба ѕ.  \n",
    "3D околината се состои од 13 влезови што ја опишуваат моменталната состојба на ракетата:\n",
    "- x позиција (лево/десно)\n",
    "- у позиција (право/позади)\n",
    "- z позиција (нагоре/надолу)\n",
    "- vх линеарна брзина по x\n",
    "- vy линеарна брзина по y\n",
    "- vz линеарна брзина по z\n",
    "- wх аголна брзина по x - агол на валање (roll)\n",
    "- wy аголна брзина по y - агол на скршнување (pitch)\n",
    "- wz аголна брзина по z - агол на извишување (yaw)\n",
    "- кватернион w (ротациски дел)\n",
    "- кватернион х\n",
    "- кватернион y\n",
    "- кватернион z\n",
    "  \n",
    "Има 6 дискретни излези:\n",
    "- 0: не прави ништо\n",
    "- 1: активирај го главниот мотор\n",
    "- 2: активирај го десниот мотор\n",
    "- 3: активирај го левиот мотор\n",
    "- 4: активирај го предниот мотор \n",
    "- 5: активирај го задниот мотор\n",
    "  \n",
    "Невронската мрежа се состои од следниве слоеви:\n",
    "- Влезен слој 13 -> 518 неврони: ги трансформира 13-те влезни вредности во 512 димензионална репрезентација\n",
    "- Скриен слој 512 -> 256: го намалуваме бројот бидејќи 512 е добро на почеток поради тоа што гледа многу различни шеми, а после 256 ја принудува мрежата да ги компресира тие податоци и да научи попрефинета репрезентација.\n",
    "- Излезен слој 256 -> 6: дава една Q вредност по можна акција\n",
    "- Нормализација: го нормализира излезот од претходниот слој, корисно е за високодимензионални и динамични состојби. Помага во стабилизирање на учењето.\n",
    "- Активациска функција ReLU: се користи за да се воведе нелинеарност. Ова ѝ овозможува на мрежата да учи сложени шеми\n",
    "\n",
    "Со методот „forward“ PyTorch знае како да ја води мрежата напред: зема состојба х и дава Q вредности за секоја акција. На пример ако состојбите на ракетата се:  \n",
    "\n",
    "x = [0.1, 0.5, 12.3, 0.99, 0.01, 0.0, 0.01, -0.2, 0.0, -0.1, 0.0, 0.0, 0.01]\n",
    "\n",
    "Тогаш  \n",
    "\n",
    "model = DQN(13, 6)  \n",
    "output = model.forward(x)  \n",
    "\n",
    "Ќе даде нешто вакво  \n",
    "\n",
    "output = [2.1, -0.5, 0.7, 1.4, -1.2, 0.3]\n",
    "\n",
    "Што значи:  \n",
    "- Q вредност не прави ништо: 2.1\n",
    "- Q вредност активирај го главниот мотор: -0.5\n",
    "- Q вредност активирај го десниот мотор: 0.7\n",
    "- Q вредност активирај го левиот мотор: 1.4\n",
    "- Q вредност активирај го предниот мотор : -1.2\n",
    "- Q вредност активирај го задниот мотор: 0.3\n",
    "\n",
    "Агентот ќе ја избере акцијата со најголема Q вредност што во случајов е да не прави ништо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8ab657-8fe9-4c68-8f07-3aa57cf5c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35f359-3541-42e0-9d42-d4963efbae8e",
   "metadata": {},
   "source": [
    "Во Reinforcement Learning агентите учат од своите искуства (состојба, акција, награда, следна состојба и сл.). Но, ако учиме од нив по редоследот по којшто се случуваат, постои проблем каде што последователните искуства се многу корелирани, па, наместо веднаш да учиме, ние ги складираме искуствата во бафер и потоа случајно земаме примероци за да ја тренираме мрежата. Ова го надминува проблемот на корелација и го прави учењето поефикасно. \n",
    "\n",
    "Во иницијализацијата користиме deque (double-ended-queue) за да ги зачуваме искуствата. maxlen=capacity осигурува дека баферот не расте бесконечно, старите искуствата автоматски се отфрлаат кога ќе се наполни баферот.\n",
    "\n",
    "Push методот додава едно искуство во баферот. Секое искуство е торка од 6 елементи: (self, state, action, reward, next_state, done). Секој пат кога агентот прави чекор во епизодата, тука го собираме неговото искуство.\n",
    "\n",
    "Во sample случајно избираме искуства од batch_size од баферот. zip(*batch) ги дели торките во посебни листи: една листа од состојби, една од акции и сл. Ова ни овозможува да учиме од повеќе минати искуства одеднаш. Потоа, ги форматираме податоците во numpy низи за да можеме да ги конвертираме во тензори."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc4f107-d20d-4bee-a19f-247df943493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states),\n",
    "                np.array(dones, dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81c06a-3bde-4b22-9233-1c506a8d3859",
   "metadata": {},
   "source": [
    "Креираме нова gym околина.\n",
    "\n",
    "init методот ги гради ракетата, теренот и светот. Целиот 3Д свет е дефиниран користејќи MuJoCo XML:\n",
    "- hfield: нерамен терен (128х128 heightmap)\n",
    "- launch_pad: мала подигната платформа\n",
    "- ракета дефинирана со:\n",
    "  - главно тело во форма на цилиндар\n",
    "  - врв на капсула\n",
    "  - 3 ногарки на капсулата\n",
    "\n",
    "Кога ќе ја вчитаме ракетата во MuJoCo имаме пристап до:\n",
    "- self.data.qpos: позиција и ориентација на ракетата\n",
    "- self.data.qvel: линеарна и аголна брзина\n",
    "- self.data.qfrc_applied: примена на сили\n",
    "\n",
    "Во reinforcement learning reset() прави 3 работи:\n",
    "- ја става околината во нова состојба\n",
    "- прави некои случајни работи (за да поттикне генерализација)\n",
    "- ја враќа почетната опсервација на агентот\n",
    "\n",
    "Методот reset() ги ресетира сите податоци за симулација во MuJoCo како да се почнува од 0, но истовремено додава и случајни иницијални линеарна и аголна брзина. Без овие случајни брзини ракетата секогаш би стартувала совршено мирно и би научила да слетува само од идеална состојба. Со оваа случајност агентот учи да се опорави од случајни реални ситуации (како на пр. талкање настрана, превртување) што го прави агентот многу попаметен и погенерализиран.\n",
    "\n",
    "Со помош на get_obs() го добиваме влезот во невронската мрежа, односно комплетната обсервација (набљудување) што го има направено лендерот. Кватернионите мора да ги нормализираме пред да ги користиме. \n",
    "\n",
    "Функцијата step() прави 5 големи работи:\n",
    "1. Ја претвора акцијата на агентот во сила\n",
    "2. Ја применува таа сила во MuJoCo\n",
    "3. Ја унапредува симулацијата за еден чекор\n",
    "4. Ја пресметува наградата\n",
    "5. Проверува дали епизодата е завршена\n",
    "\n",
    "Прво, ориентацијата на ракетата ја претвораме во 3х3 матрица на ротација така што можеме да примениме сила во вистинската насока, дури и ако е навалена ракетата. Односно, ако директно сакаме да примениме сила по z-оска на пример, доколку ракетата е навалена силата ќе влијае во погрешна насока бидејќи таа ќе ја гледа z-оската во однос на нејзиниот свет, не глобалниот. Затоа ни треба матрицата на ротација. Таа опишува како да се ротира вектор од еден координатен систем во друг.\n",
    "\n",
    "compute_reward() пресметува колку добро се справува агентот врз основа на неговата позиција, брзина, ориентација и растојание од платформата. \n",
    "- tilt_penalty: ја казнува ракетата за преголемо навалување. Ако qx и qy се 0, таа е совршено исправена\n",
    "- dist: колу е оддалечена ракетата од центарот на платформата\n",
    "- vel_horizontal: колку брзо се движи од страна на страна\n",
    "- vel_vertical: колку брзо се движи нагоре/надолу\n",
    "- altitude_reward: колку е блиску до платформата\n",
    "- position_reward: целта е ракетата да е поставена директно над платформата\n",
    "- velocity_reward: движи се бавно и сигурно кога слетуваш\n",
    "- orientation_reward: остани исправена\n",
    "\n",
    "is_done се користи за да дознаеме дали обидот на ракетата да слета заврши.\n",
    "\n",
    "render() функцијата визуелно ја прикажува симулацијата. Отвора нов прозорец и ја прикажува ракетата како лета наоколу во 3Д. Кога ќе заврши визуелизацијата, се затвора со close()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8065ca15-fe55-403d-bc1e-25f0ce7bd323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLander3D(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self, render_mode=\"human\"):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        xml = \"\"\"\n",
    "        <mujoco>\n",
    "          <asset>\n",
    "            <hfield name=\"terrain\" nrow=\"128\" ncol=\"128\" size=\"40 40 2 0.1\"/>\n",
    "            <material name=\"moon_soil\" rgba=\"0.3 0.3 0.3 1\"/>\n",
    "            <material name=\"launch_pad\" rgba=\"0.8 0.1 0.1 1\"/>\n",
    "            <material name=\"rocket_body\" rgba=\"0.9 0.9 0.9 1\"/>\n",
    "            <material name=\"legs\" rgba=\"0.2 0.2 0.2 1\"/>\n",
    "          </asset>\n",
    "          <worldbody>\n",
    "            <light directional=\"true\" diffuse=\"0.9 0.9 0.9\" pos=\"0 0 10\"/>\n",
    "            <geom type=\"hfield\" hfield=\"terrain\" material=\"moon_soil\" pos=\"0 0 0\"/>\n",
    "            <body pos=\"0 0 0.5\">\n",
    "              <geom type=\"cylinder\" size=\"2 0.3\" material=\"launch_pad\"/>\n",
    "            </body>\n",
    "            <body name=\"lander\" pos=\"0 0 15\">\n",
    "              <joint name=\"root\" type=\"free\"/>\n",
    "              <geom type=\"cylinder\" size=\"0.4 6\" pos=\"0 0 0\" material=\"rocket_body\"/>\n",
    "              <geom type=\"capsule\" fromto=\"0 0 6 0 0 8\" size=\"0.3\" material=\"rocket_body\"/>\n",
    "              <geom type=\"capsule\" fromto=\"0.6 0 -5.5 1.0 0 -8.0\" size=\"0.1\" material=\"legs\"/>\n",
    "              <geom type=\"capsule\" fromto=\"-0.3 0.519 -5.5 -0.6 1.038 -8.0\" size=\"0.1\" material=\"legs\"/>\n",
    "              <geom type=\"capsule\" fromto=\"-0.3 -0.519 -5.5 -0.6 -1.038 -8.0\" size=\"0.1\" material=\"legs\"/>\n",
    "              <geom type=\"sphere\" size=\"0.15\" pos=\"0.6 0 -5.5\" material=\"legs\"/>\n",
    "              <geom type=\"sphere\" size=\"0.15\" pos=\"-0.3 0.519 -5.5\" material=\"legs\"/>\n",
    "              <geom type=\"sphere\" size=\"0.15\" pos=\"-0.3 -0.519 -5.5\" material=\"legs\"/>\n",
    "            </body>\n",
    "          </worldbody>\n",
    "        </mujoco>\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = mujoco.MjModel.from_xml_string(xml)\n",
    "        self._generate_lunar_terrain()\n",
    "        self.data = MjData(self.model)\n",
    "        self.viewer = None\n",
    "\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf, (13,), dtype=np.float32)\n",
    "        self.max_steps = 1000\n",
    "        self.step_count = 0\n",
    "\n",
    "    def _generate_lunar_terrain(self):\n",
    "        heightmap = np.zeros((128, 128), dtype=np.float32)\n",
    "        for _ in range(5):\n",
    "            x, y = np.random.randint(20, 108), np.random.randint(20, 108)\n",
    "            size = np.random.uniform(5, 15)\n",
    "            depth = np.random.uniform(0.5, 2.0)\n",
    "            xx, yy = np.mgrid[:128, :128]\n",
    "            dist = np.sqrt((xx - x) ** 2 + (yy - y) ** 2)\n",
    "            heightmap += depth * np.exp(-dist ** 2 / (2 * (size ** 2)))\n",
    "        heightmap += np.random.rand(128, 128) * 0.3\n",
    "        heightmap[60:68, 60:68] = heightmap.min()\n",
    "        heightmap = (heightmap - heightmap.min()) / (heightmap.max() - heightmap.min())\n",
    "        heightmap = heightmap * 1.5 + 0.1\n",
    "        self.model.hfield_data[:] = heightmap.ravel()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "        self.step_count = 0\n",
    "    \n",
    "        # Linear velocities\n",
    "        self.data.qvel[0] = self.np_random.uniform(-3.0, 3.0)  # X velocity (sideways)\n",
    "        self.data.qvel[1] = self.np_random.uniform(-3.0, 3.0)  # Y velocity (forward/back)\n",
    "        self.data.qvel[2] = self.np_random.uniform(-1.0, 1.0)  # Z velocity (up/down)\n",
    "        \n",
    "        ## Angular velocities\n",
    "        #self.data.qvel[3] = self.np_random.uniform(-0.5, 0.5)  # X-axis rotation (roll)\n",
    "        #self.data.qvel[4] = self.np_random.uniform(-0.5, 0.5)  # Y-axis rotation (pitch)\n",
    "        #self.data.qvel[5] = self.np_random.uniform(-0.5, 0.5)  # Z-axis rotation (yaw)\n",
    "        # No angular velocity — don't spin\n",
    "        self.data.qvel[3:6] = 0\n",
    "\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Position (x, y, z) and quaternion\n",
    "        position = self.data.qpos[0:3]\n",
    "        quat = self.data.qpos[3:7]\n",
    "    \n",
    "        # Normalize quaternion\n",
    "        quat /= np.linalg.norm(quat)\n",
    "    \n",
    "        # Velocity (linear and angular)\n",
    "        linear_vel = self.data.qvel[0:3]\n",
    "        angular_vel = self.data.qvel[3:6]\n",
    "    \n",
    "        return np.concatenate([\n",
    "            position,\n",
    "            quat,\n",
    "            linear_vel,\n",
    "            angular_vel\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get body rotation matrix\n",
    "        body_quat = self.data.body(\"lander\").xquat  # [w, x, y, z] format\n",
    "        body_rot = np.zeros(9)\n",
    "        mujoco.mju_quat2Mat(body_rot, body_quat)\n",
    "        rotation_matrix = body_rot.reshape(3, 3)\n",
    "        \n",
    "        # Apply thrust in body frame\n",
    "        if action == 1:          # main\n",
    "            body_thrust = np.array([  0.0,   0.0, -180.0])\n",
    "        elif action == 2:        # right\n",
    "            body_thrust = np.array([ 60.0,   0.0,    0.0])\n",
    "        elif action == 3:        # left\n",
    "            body_thrust = np.array([-60.0,   0.0,    0.0])\n",
    "        elif action == 4:        # forward (+y)\n",
    "            body_thrust = np.array([  0.0,  60.0,    0.0])\n",
    "        elif action == 5:        # back (−y)\n",
    "            body_thrust = np.array([  0.0, -60.0,    0.0])\n",
    "        else:                    # action == 0\n",
    "            body_thrust = np.zeros(3)\n",
    "\n",
    "        global_thrust = rotation_matrix @ body_thrust\n",
    "\n",
    "    \n",
    "        # Apply the transformed force\n",
    "        mujoco.mj_applyFT(\n",
    "            self.model, self.data, \n",
    "            global_thrust, \n",
    "            np.zeros(3),\n",
    "            np.zeros(3),\n",
    "            mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, b\"lander\"), \n",
    "            self.data.qfrc_applied\n",
    "        )\n",
    "        \n",
    "        # Step the simulation and return values\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        reward = self._compute_reward(obs)\n",
    "        done = self._is_done(obs)\n",
    "        \n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "    def _compute_reward(self, obs):\n",
    "        x, y, z = obs[0:3]\n",
    "        body_z = z\n",
    "        feet_z = body_z - 8.0  # Adjust for body-foot offset\n",
    "\n",
    "        # Quaternion components (already normalized)\n",
    "        qw, qx, qy, qz = obs[3:7]\n",
    "        lin_vel = obs[7:10]\n",
    "        ang_vel = obs[10:13]\n",
    "        \n",
    "        # Use 2*(qx^2 + qy^2) for tilt (0=upright, 1=horizontal)\n",
    "        tilt_penalty = 2 * (qx**2 + qy**2)\n",
    "        \n",
    "        dist = np.sqrt(x**2 + y**2)\n",
    "        vel_horizontal = np.linalg.norm(lin_vel[:2])\n",
    "        vel_vertical = lin_vel[2]\n",
    "\n",
    "        # Base rewards\n",
    "        altitude_reward = np.exp(-0.5 * abs(feet_z - 0.5))  # Peak at pad height\n",
    "        position_reward = np.exp(-2 * dist)  # around pad\n",
    "        velocity_reward = np.exp(-4 * vel_horizontal) - 0.5 * abs(vel_vertical)\n",
    "        orientation_reward = 1 - tilt_penalty\n",
    "    \n",
    "        reward = (\n",
    "            2.0 * altitude_reward +\n",
    "            1.5 * position_reward +\n",
    "            1.0 * velocity_reward +\n",
    "            0.8 * orientation_reward\n",
    "        )\n",
    "        \n",
    "        # Success bonus (upright, low speed, on pad)\n",
    "        if (feet_z < 0.3 and dist < 0.5 and \n",
    "            vel_horizontal < 0.2 and abs(vel_vertical) < 0.3 and\n",
    "            tilt_penalty < 0.1):\n",
    "            reward += 200\n",
    "        # Off-pad but landed gently = penalty\n",
    "        elif feet_z < 0.3 and dist >= 0.5:\n",
    "            reward -= 50\n",
    "\n",
    "        \n",
    "        # Crash penalty\n",
    "        if feet_z < 0:\n",
    "            reward -= 100\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def _is_done(self, obs):\n",
    "        feet_z = obs[2] - 8.0\n",
    "        return feet_z < 0 or self.step_count >= self.max_steps\n",
    "    \n",
    "    def render(self):\n",
    "        if self.viewer is None:\n",
    "            self.viewer = mujoco_viewer.MujocoViewer(self.model, self.data)\n",
    "            self.viewer.cam.distance = 25.0\n",
    "            self.viewer.cam.azimuth = 35\n",
    "            self.viewer.cam.elevation = -25\n",
    "        self.viewer.render()\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f9cc3-7884-483e-b903-8e4341f5fb1f",
   "metadata": {},
   "source": [
    "Класата Normalizer ги следи тековната средна вредност и стандардната девијација на состојбите и ги нормализира. Зошто да се нормализира? Невронските мрежи учат подобро кога влезните вредности се центрирани околу 0 и се во разумен опсег (не премногу големи, не премногу мали). Ако на пр. z=15 и qx=0.02 мрежата ќе се бори да ги балансира скаливе, освен ако не се нормализира.\n",
    "\n",
    "update ги ажурира овие податоци базирано на нова обсервација x.\n",
    "\n",
    "normalize ја центрира вредноста со тоа што ја одзема средната вредност и ја скалира вредноста со тоа што ја дели со std. 1e-8 се додава за да се спречи делење со 0. Резултат: влезот има средна вредност приближна на 0 и std приближно на 1 што е подобро за тренирање."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b7a8b7-2575-428d-9460-6fce0e42922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, size):\n",
    "        self.n = 0                       # n - number of updates so far\n",
    "        self.mean = np.zeros(size)       # mean - mean vector (starts at 0)\n",
    "        self.var = np.ones(size)         # var - variance (starts at 1)\n",
    "        self.std = np.sqrt(self.var)     # std - standard deviation (sqrt of variance)\n",
    "\n",
    "    def update(self, x):\n",
    "        self.n += 1\n",
    "        new_mean = self.mean + (x - self.mean) / self.n\n",
    "        new_var = self.var + (x - self.mean) * (x - new_mean)\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.std = np.sqrt(self.var / self.n) if self.n > 1 else np.ones_like(x)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / (self.std + 1e-8)  # Prevent division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92200292-8fb0-46b1-8007-b8bb649e6de6",
   "metadata": {},
   "source": [
    "Класата Trainer поставува 3D околина, иницијализира мрежи, бафер и оптимизатор, тренира, исцртува награди, го визуелизира агентот...\n",
    "\n",
    "init() ја иницијализира 3Д околината. Со render_mode=None викаме дека нема прозорец за прегледување (побрзо се тренира). Ги оптимизира тежините со помош на Адам.\n",
    "\n",
    "policy() ја скалира 13 димензионалната состојба да има средна вредност 0 и стандардна девијација 1. Осигурува дека различни влезни карактеристики (како позиција, брзина) нема да доминираат во учењето. Во раните фази на тренирање, истражувањето е поттикнато бидејќи имаме веројатност епсилон за случајно дејство. Инаку, се избира најпознатата акција од Q мрежата. Нормализираната состојба се префрла во PyTorch тензор и се прави unsqueeze(0) за обликот да стане [1,13]. Тука немаме градиенти бидејќи не тренираме, само евалуираме. На крај, argmax(dim=1) ја избира акцијата со највисока предвидена Q одговорност. item() го претвора тензорот во обичен број.\n",
    "\n",
    "update_model() прво проверува дали има доволно примероци за тренирање, ако баферот не е полн не тренираме. Земаме batch, нормализираме и конвертираме во PyTorch тензори. Потоа, ги предвидуваме сите Q вредности од моменталните состојби. Користиме .gather() за да ги земеме Q вредностите за преземените акции. Користиме Double DQN со тоа што земаме argmax од policy_net (кое дејство би го избрале), но добиваме вредност од target_net (постабилно е). Грешката се рачуна како MSE на предвидените Q вредности и целните Q вредности. Тука имаме и бекпропагација со тоа што ги чистиме старите градиенти, калкулираме нови, овие градиенти и кажуваат на мрежата како да ги промени своите тежини за да се намали загубата, ги кратиме градиенти за да не станат преголеми и ги ажурираме тежините.\n",
    "\n",
    "Во train() е целата логика за тренирање на моделот. Се повикуваат претходно објаснетите функции и на секои 20 епизоди се визуелизира дотогап најдоброто слетување на ракетата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21473f08-1dc6-4ba5-ab5c-1653b8704c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2a7d162a7b4edb9df66e3fa2a94b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='danger', description='Stop Training', style=ButtonStyle()), FigureWidget({…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Model saved at Episode 0 with Reward 123.96\n",
      "New Best Model saved at Episode 1 with Reward 264.33\n",
      "New Best Model saved at Episode 3 with Reward 844.75\n",
      "New Best Model saved at Episode 120 with Reward 960.59\n",
      "New Best Model saved at Episode 201 with Reward 1276.46\n",
      "New Best Model saved at Episode 760 with Reward 1676.48\n",
      "New Best Model saved at Episode 841 with Reward 1814.18\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.env = LunarLander3D(render_mode=None)  # Disable rendering during training\n",
    "        self.policy_net = DQN(13, 6).to(device)\n",
    "        self.target_net = DQN(13, 6).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=ALPHA)\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.best_reward = -float('inf')\n",
    "        self.should_stop = False\n",
    "        self.rewards = []\n",
    "        self.setup_dashboard()\n",
    "        self.normalizer = Normalizer(13)\n",
    "\n",
    "    def policy(self, state):\n",
    "        normalized_state = self.normalizer.normalize(state)\n",
    "        # ε-greedy over Discrete(4)\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()   # random int in [0..3]\n",
    "        # else pick argmax Q\n",
    "        state_tensor = torch.FloatTensor(normalized_state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.policy_net(state_tensor)  # shape [1,4]\n",
    "        return int(q_vals.argmax(dim=1).item())\n",
    "\n",
    "\n",
    "    def setup_dashboard(self):\n",
    "        self.fig = go.FigureWidget()\n",
    "        self.fig.add_scatter(x=[], y=[], mode='markers+lines')\n",
    "        self.fig.layout.title = '3D Lunar Lander Training'\n",
    "        self.fig.layout.xaxis.title = 'Episode'\n",
    "        self.fig.layout.yaxis.title = 'Reward'\n",
    "        \n",
    "        self.stop_button = widgets.Button(description='Stop Training', button_style='danger')\n",
    "        self.stop_button.on_click(lambda b: setattr(self, 'should_stop', True))\n",
    "        \n",
    "        self.dashboard = widgets.VBox([\n",
    "            self.stop_button,\n",
    "            self.fig\n",
    "        ])\n",
    "\n",
    "    def update_model(self):\n",
    "        # Don’t train until we have at least one full batch\n",
    "        if len(self.replay_buffer.buffer) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Sample and normalize\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        # Normalize all states\n",
    "        normalized_states = np.array([self.normalizer.normalize(s) for s in states])\n",
    "        normalized_next_states = np.array([self.normalizer.normalize(s) for s in next_states])\n",
    "\n",
    "        states      = torch.FloatTensor(normalized_states).to(device)\n",
    "        actions     = torch.LongTensor(actions).view(-1,1).to(device)    \n",
    "        rewards     = torch.FloatTensor(rewards).unsqueeze(1).to(device) \n",
    "        next_states = torch.FloatTensor(normalized_next_states).to(device)\n",
    "        dones       = torch.FloatTensor(dones).unsqueeze(1).to(device)   \n",
    "\n",
    "        q_vals    = self.policy_net(states)                       \n",
    "        current_q = q_vals.gather(1, actions)                     \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN: Use policy net for action, target net for value\n",
    "            next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "            max_next_q = self.target_net(next_states).gather(1, next_actions)\n",
    "            target_q = rewards + GAMMA * max_next_q * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        self.optimizer.zero_grad()  # clear old gradients\n",
    "        loss.backward()  # backpropagation\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)  # clip gradients so they don't get too large\n",
    "        self.optimizer.step()   # update weights\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        display(self.dashboard)\n",
    "        \n",
    "        # Initialize normalizer with first state\n",
    "        state, _ = self.env.reset()\n",
    "        self.normalizer.update(state)  # Initial state\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            if self.should_stop:\n",
    "                print(\"Training stopped by user\")\n",
    "                break\n",
    "                \n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done and not self.should_stop:\n",
    "                # Normalize state before passing to policy\n",
    "                normalized_state = self.normalizer.normalize(state)\n",
    "                action = self.policy(normalized_state)  # Policy now uses normalized state\n",
    "                \n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                # Update normalizer with new state\n",
    "                self.normalizer.update(next_state)\n",
    "                \n",
    "                self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                self.update_model()\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "            \n",
    "            for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "                target_param.data.copy_(TAU * policy_param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "            if total_reward > self.best_reward:\n",
    "                self.best_reward = total_reward\n",
    "                torch.save(self.policy_net.state_dict(), \"best_3d_lander.pth\")\n",
    "                print(f\"New Best Model saved at Episode {episode} with Reward {total_reward:.2f}\")\n",
    "\n",
    "            # Update dashboard\n",
    "            with self.fig.batch_update():\n",
    "                self.fig.data[0].x = tuple(list(self.fig.data[0].x) + [episode])\n",
    "                self.fig.data[0].y = tuple(list(self.fig.data[0].y) + [total_reward])\n",
    "\n",
    "            # Visualize in main thread\n",
    "            if episode % 20 == 0:\n",
    "                self.visualize_agent()\n",
    "                \n",
    "        self.env.close()\n",
    "\n",
    "    def visualize_agent(self):\n",
    "        preview_env = LunarLander3D(render_mode=\"human\")\n",
    "        # load best weights once\n",
    "        self.policy_net.load_state_dict(torch.load(\"best_3d_lander.pth\"))\n",
    "        self.policy_net.eval()\n",
    "\n",
    "        state, _ = preview_env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # ε=0 greedy\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                action = int(self.policy_net(state_tensor).argmax().item())\n",
    "\n",
    "            # step + render\n",
    "            state, reward, done, _, _ = preview_env.step(action)\n",
    "            preview_env.render()\n",
    "\n",
    "            # cap FPS\n",
    "            import time\n",
    "            time.sleep(1/60)\n",
    "\n",
    "        preview_env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = Trainer()\n",
    "    trainer.train(num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4943b9a-c763-4ea0-8869-dabc6d40f38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
